import os
import ast
import csv
import spacy
import argparse
import numpy as np
import pandas as pd

from pathlib import Path
from textacy import extract
from spacy.tokens import Doc
from spacy.attrs import HEAD, DEP
from typing import List, Tuple, Dict, Any, Optional, Set
from config import TOKENS, ENTITY, BASE_OUT_DIR, BOOK, TRIPLE_OUT
from auxiliary import load_booknlp_file, print_headers, print_information, safe_to_list


# ================= CONFIG =================
VALIDATE_TREES = False

nlp = spacy.blank("en")

# =========== spaCy.DOC RECONSTRUCTION ===========
def make_doc_from_sentence(sentence: pd.DataFrame, validate=False) -> Doc:
    """
    Creates a `spacy.Doc`-object given a sentence extracted from
    the `.tokens`-file generated by BookNLP. 

    Working with `spacy.Doc`-objects allows for building dependency
    trees and relationships between syntactical components of a
    sentence.
    
    :param sentence: The sentence to process further as a `spacy.Doc`-object
    :type sentence: pd.DataFrame
    :param validate: Whether to validate the creation of the `spacy.Doc`-object
    :return: Returns the `spacy.Doc` version of the sentence
    :rtype: spacy.Doc
    """
    # Extract all words in the sentence & compute location of spaces
    words  = sentence["word"].tolist()
    spaces = [True] * (len(words) - 1) + [False]

    # Create custom spacy.Doc object for the current sentence
    doc = Doc(nlp.vocab, words=words, spaces=spaces)

    # Compute sentence-local token IDs. Required by spacy.Doc()
    global_to_local = {row["token_ID_within_document"]: idx for idx, (_, row) in enumerate(sentence.iterrows())}
    
    heads = []
    deps  = []

    # Add token indices relative to sentence head &
    # add dependencies to the vocabulary
    for idx, (_, row) in enumerate(sentence.iterrows()):
        global_head_idx = int(row["syntactic_head_ID"])        
        local_head_idx = global_to_local[global_head_idx]
        rel_head = local_head_idx - idx
        
        heads.append(rel_head)
        deps.append(nlp.vocab.strings.add(row["dependency_relation"]))    
    
    n = len(heads)
    arr = np.zeros((n, 2), dtype="uint64")

    for i in range(n):
        arr[i, 0] = np.int64(heads[i]).view(np.uint64)
        arr[i, 1] = deps[i]

    # Add head and dependency information to Doc-object
    doc.from_array([HEAD, DEP], arr)

    # Assign POS / TAG / LEMMA and store global token IDs
    for token, (_, row) in zip(doc, sentence.iterrows()):
        token.pos_   = row["POS_tag"]
        token.tag_   = row["fine_POS_tag"]
        token.lemma_ = row["lemma"]
        # Store global token ID as custom attribute
        token._.global_id = row["token_ID_within_document"]

    if validate:
        validate_doc(doc, sentence["sentence_ID"].iloc[0])

    return doc

# ================ VALIDATOR ===============
def validate_doc(doc, sentence_id):
    roots = [t for t in doc if t.head == t]
    if len(roots) != 1:
        raise ValueError(
            f"[VALIDATION ERROR] sentence {sentence_id}: "
            f"{len(roots)} ROOTs detected"
        )

    branching = [t for t in doc if len(list(t.children)) > 1]
    if not branching:
        raise ValueError(
            f"[VALIDATION ERROR] sentence {sentence_id}: "
            f"no token has more than one child"
        )

    for token in doc:
        if token.head == token and token.dep_ != "ROOT":
            raise ValueError(
                f"[VALIDATION ERROR] sentence {sentence_id}: "
                f"non-ROOT self-cycle at token '{token.text}'"
            )


# ============ MATCH NAMES ============
def build_token_to_character_map(entities_df: pd.DataFrame) -> Dict[int, int]:
    """
    Build a mapping from token IDs to character IDs using the entities file.
    
    The entities file contains COREF IDs that link tokens to characters.
    For each entity mention (row), we map all tokens in the range 
    [start_token, end_token] to the COREF ID.
    
    Returns:
        Dict mapping token_id -> character_id (COREF)
    """
    token_to_char = {}
    
    for _, row in entities_df.iterrows():
        coref_id = row['COREF']
        start_token = int(row['start_token'])
        end_token = int(row['end_token'])
        
        # Map all tokens in this entity mention to the character ID
        for token_id in range(start_token, end_token + 1):
            token_to_char[token_id] = coref_id
    
    return token_to_char


def map_token_ids_to_coref(token_ids_str: str, token_to_char_id: Dict[int, int]) -> List:
    """
    Map a list of token IDs (as string) to their corresponding COREF character IDs.
    Returns a set of unique character IDs.
    """
    
    try:
        token_ids = ast.literal_eval(token_ids_str)
    except (ValueError, SyntaxError):
        return []
    
    # Map each token ID to its COREF ID (if exists) and collect unique values
    coref_ids = set()
    for token_id in token_ids:
        if token_id in token_to_char_id:
            coref_ids.add(token_to_char_id[token_id])
    return list(coref_ids)


def match_list_to_canonical(ids_list, original_to_canonical):
    """Match a list of COREF IDs to canonical (id, fullname)."""
    for coref_id in ids_list:
        if coref_id in original_to_canonical:
            return original_to_canonical[coref_id]
    return (None, None)


def match_ids_to_canonical(ids_str: str, original_to_canonical: Dict[int, Tuple[int, str]]) -> Tuple[Optional[int], Optional[str]]:
            """
            Given a set of COREF IDs (as string), find the canonical match.
            Returns (canonical_id, fullname) or (None, None) if no match.
            """
            try:
                # Handle both set and list string representations
                ids = ast.literal_eval(ids_str)
                if isinstance(ids, set):
                    ids = list(ids)
            except (ValueError, SyntaxError):
                return None, None
            
            for coref_id in ids:
                if coref_id in original_to_canonical:
                    return original_to_canonical[coref_id]
            return None, None


def match_to_canonical_names(svo: pd.DataFrame, verbose: bool = False) -> Tuple[pd.DataFrame, Dict]:
    """
    Match SVO subject/object IDs to canonical character names.
    
    If canonical_mappings.csv exists:
        - Filter out rows with avg_name_match_score == 0.0
        - Match subject_ids/object_ids to original_ids in canonical_mappings
        - Replace with fullname and canonical_id
    
    If canonical_mappings.csv doesn't exist:
        - Call match_name() from match_names.py to generate mappings
    
    Returns:
        - DataFrame with only rows where both subject and object were matched
        - Dict with name matching statistics
    """
    canonical_path = f"{BASE_OUT_DIR}/canonical_mappings.csv"
    
    if os.path.exists(canonical_path):
        print_information('Using existing canonical_mappings.csv...', 6, '\n')
        
        # Load canonical mappings
        canonical_df = pd.read_csv(canonical_path)
        
        # Load names.csv for gender lookup (match by fullname)
        names_df = pd.read_csv('../data/names.csv')
        name_to_gender = dict(zip(names_df['fullname'], names_df['gender']))
        
        # Remove entries with no match (avg_name_match_score == 0.0)
        canonical_df = canonical_df[canonical_df['avg_name_match_score'] != 0]
        
        # Build lookup: original_id -> (canonical_id, fullname)
        # original_ids is stored as a string representation of a list
        original_to_canonical = {}
        for _, row in canonical_df.iterrows():
            canonical_id = row['canonical_id']
            fullname = row['fullname']
            original_ids = ast.literal_eval(row['original_ids'])
            for orig_id in original_ids:
                original_to_canonical[orig_id] = (canonical_id, fullname)
        
        # Preserve original text as name variants (before replacing with canonical fullname)
        svo['subj_name_variant'] = svo['subject_text'].copy()
        svo['obj_name_variant'] = svo['object_text'].copy()

        # Store original names as list for downstream compatibility w/ AVP triples
        svo['subj_name_variant'] = svo['subject_text'].apply(lambda x: '[\'' + str(x) + '\']' if x is not None else '[]')
        svo['obj_name_variant'] = svo['object_text'].apply(lambda x: '[\'' + str(x) + '\']' if x is not None else '[]')
        
        # Preserve original COREF IDs as lists
        svo['original_subj_ids'] = svo['subject_ids'].apply(safe_to_list)
        svo['original_obj_ids'] = svo['object_ids'].apply(safe_to_list)
        
        # Apply matching to subjects
        subject_matches = svo['original_subj_ids'].apply(lambda x: match_list_to_canonical(x, original_to_canonical))
        svo['canonical_subj_id'] = subject_matches.apply(lambda x: int(x[0]) if x[0] is not None else None)
        svo['subject_text'] = svo.apply(
            lambda row: subject_matches[row.name][1] if subject_matches[row.name][1] else row['subject_text'], 
            axis=1
        )
        
        # Apply matching to objects
        object_matches = svo['original_obj_ids'].apply(lambda x: match_list_to_canonical(x, original_to_canonical))
        svo['canonical_obj_id'] = object_matches.apply(lambda x: int(x[0]) if x[0] is not None else None)
        svo['object_text'] = svo.apply(
            lambda row: object_matches[row.name][1] if object_matches[row.name][1] else row['object_text'], 
            axis=1
        )
        
        # Drop the old subject_ids and object_ids columns
        svo = svo.drop(columns=['subject_ids', 'object_ids'])
        
        # Remove rows where either subject or object wasn't matched
        matched_mask = svo['canonical_subj_id'].notna() & svo['canonical_obj_id'].notna()
        unmatched_count = (~matched_mask).sum()
        svo = svo[matched_mask]
        
        # Convert canonical IDs to integers (after filtering out NaNs)
        svo['canonical_subj_id'] = svo['canonical_subj_id'].astype(int)
        svo['canonical_obj_id'] = svo['canonical_obj_id'].astype(int)
        
        # Add gender columns by looking up fullnames in names.csv
        svo['subj_gender'] = svo['subject_text'].map(name_to_gender)
        svo['obj_gender'] = svo['object_text'].map(name_to_gender)
        
        # Add source and role columns
        svo['source'] = 'svo'
        svo['subj_role'] = 'subj'
        svo['obj_role'] = 'obj'
        
        # Rename verb columns for consistency
        svo = svo.rename(columns={
            'verb_text': 'word',
            'verb_lemma': 'lemma'
        })
        
        # Reorder columns to match requested format
        svo = svo[[
            'source', 'canonical_subj_id', 'subject_text', 'subj_role', 
            'word', 'lemma', 'verb_id', 'negated', 'original_subj_ids', 'subj_gender', 'subj_name_variant',
            'canonical_obj_id', 'object_text', 'obj_role', 'original_obj_ids', 'obj_gender', 'obj_name_variant'
        ]]
        
        
        # Collect name matching statistics
        name_stats = {
            'triples_before_matching': len(svo) + unmatched_count,
            'triples_after_matching': len(svo),
            'unmatched_count': unmatched_count,
            'unique_subjects': svo['canonical_subj_id'].nunique(),
            'unique_objects': svo['canonical_obj_id'].nunique(),
            'unique_characters': len(set(svo['canonical_subj_id'].unique()) | set(svo['canonical_obj_id'].unique())),
            'canonical_mappings_used': len(canonical_df)
        }
        
    else:
        print_information('canonical_mappings.csv not found. Running name matching pipeline...', 6, '\n')
        
        # Import and run the name matching pipeline
        from match_names import main as match_names_main
        match_names_main(BOOK, BASE_OUT_DIR, verbose=verbose)
        
        # Now that we have canonical_mappings, recursively call this function
        return match_to_canonical_names(svo, verbose)
    
    print_information('Finished matching names', prefix="    ")
    return svo, name_stats


# ============ SVO EXTRACTION USING TEXTACY ==============
def get_compound_tokens(token) -> List:
    """
    Get all tokens that form a compound noun with the given token.
    Returns tokens in document order.
    """
    compound_tokens = [token]
    
    # Look for compound modifiers (words with dep='compound' that point to this token)
    for child in token.children:
        if child.dep_ == "compound":
            # Recursively get compounds of the compound
            compound_tokens.extend(get_compound_tokens(child))
    
    # Sort by position in document to maintain proper order
    compound_tokens.sort(key=lambda t: t.i)
    return compound_tokens


def get_noun_info(component) -> Tuple:
    """
    Extract info for subject/object components, expanding to include compound nouns.
    This ensures proper nouns like "Miss Gabrielle Turl" are fully captured.
    """
    # Collect all tokens from the component
    if isinstance(component, list):
        base_tokens = component
    elif isinstance(component, spacy.tokens.Span):
        base_tokens = list(component)
    else:
        base_tokens = [component]
    
    # Expand each token to include its compounds
    all_tokens = []
    seen = set()
    for token in base_tokens:
        for t in get_compound_tokens(token):
            if t.i not in seen:
                all_tokens.append(t)
                seen.add(t.i)
    
    # Sort by position to maintain document order
    all_tokens.sort(key=lambda t: t.i)
    
    if not all_tokens:
        return "", "", [], None, "NOUN"
    
    ids = [t._.global_id for t in all_tokens]
    text = " ".join([t.text for t in all_tokens])
    lemma = " ".join([t.lemma_ for t in all_tokens])
    primary_id = ids[0]
    
    # Determine POS - use root of original component if available
    if isinstance(component, spacy.tokens.Span):
        pos = component.root.pos_
    elif isinstance(component, list):
        pos = component[-1].pos_ if component else "NOUN"
    else:
        pos = component.pos_
        
    return text, lemma, ids, primary_id, pos


def get_verb_info(component) -> Tuple:
    """
    Extract info for verb components, keeping only the main verb (not auxiliaries).
    Auxiliaries like 'had', 'been', 'would', 'could' are excluded.
    """
    # Collect all tokens from the component
    if isinstance(component, list):
        tokens = component
    elif isinstance(component, spacy.tokens.Span):
        tokens = list(component)
    else:
        tokens = [component]
    
    # Find the main verb(s) - tokens with POS='VERB' that are not auxiliaries
    main_verbs = [t for t in tokens if t.pos_ == "VERB" and t.dep_ not in ("aux", "auxpass")]
    
    # If no main verbs found, fall back to all non-aux tokens with VERB or AUX pos
    if not main_verbs:
        main_verbs = [t for t in tokens if t.dep_ not in ("aux", "auxpass")]
    
    # If still empty, use all tokens
    if not main_verbs:
        main_verbs = tokens
    
    # Sort by position
    main_verbs.sort(key=lambda t: t.i)
    
    if not main_verbs:
        return "", "", [], None, "VERB"
    
    ids = [t._.global_id for t in main_verbs]
    text = " ".join([t.text for t in main_verbs])
    lemma = " ".join([t.lemma_ for t in main_verbs])
    primary_id = ids[0]
    pos = main_verbs[0].pos_
    
    return text, lemma, ids, primary_id, pos


def is_negated(verb_component) -> bool:
    """
    Check if the verb is negated by looking for a child with dep_="neg".
    
    Examples of negation: "does not like", "didn't see", "never went"
    """
    # Get all tokens from the verb component
    if isinstance(verb_component, list):
        tokens = verb_component
    elif isinstance(verb_component, spacy.tokens.Span):
        tokens = list(verb_component)
    else:
        tokens = [verb_component]
    
    # Check if any verb token has a negation child
    for token in tokens:
        for child in token.children:
            if child.dep_ == "neg":
                return True
    return False


def split_into_sequential_chunks(ids) -> List:
    """
    Split a list of IDs into sequential (consecutive) chunks with their indices.
    Example: [10, 20, 21, 22, 50] -> [([10], 0, 1), ([20, 21, 22], 1, 4), ([50], 4, 5)]
    Returns list of (id_chunk, start_idx, end_idx) tuples.
    """
    if not ids:
        return [([], 0, 0)]
    
    chunks = []
    start_idx = 0
    current_chunk = [ids[0]]
    
    for i in range(1, len(ids)):
        if ids[i] == ids[i-1] + 1:
            # Sequential, add to current chunk
            current_chunk.append(ids[i])
        else:
            # Non-sequential, save current chunk with indices and start new one
            chunks.append((current_chunk, start_idx, start_idx + len(current_chunk)))
            start_idx = i
            current_chunk = [ids[i]]
    
    # Don't forget the last chunk
    chunks.append((current_chunk, start_idx, start_idx + len(current_chunk)))
    return chunks


# ================== SVO EXTRACTION ==================
def extract_svo(doc: pd.DataFrame) -> List[Dict[str, Any]]:
    svo_triples = list(extract.subject_verb_object_triples(doc))
    structured_triples = []
    
    for s, v, o in svo_triples:
        # Use get_noun_info for subjects/objects to expand compound nouns
        s_text, _, s_ids, _, s_pos = get_noun_info(s)
        # Use get_verb_info for verbs to exclude auxiliaries
        v_text, v_lemma, _, v_id, _ = get_verb_info(v)
        o_text, _, o_ids, _, o_pos = get_noun_info(o)

        # Check if the verb is negated
        negated = is_negated(v)
        
        # Split texts into word lists for slicing
        s_words = s_text.split()
        o_words = o_text.split()
        
        # Split subject and object IDs into sequential chunks with indices
        s_chunks = split_into_sequential_chunks(s_ids)
        o_chunks = split_into_sequential_chunks(o_ids)
        
        # Create Cartesian product of subject and object chunks
        for s_chunk_ids, s_start, s_end in s_chunks:
            for o_chunk_ids, o_start, o_end in o_chunks:
                structured_triples.append({
                    "subject_text": " ".join(s_words[s_start:s_end]),
                    "subject_ids":  s_chunk_ids,
                    "subject_pos":  s_pos,
                    "verb_text":    v_text,
                    "verb_lemma":   v_lemma,
                    "verb_id":      v_id,
                    "object_text":  " ".join(o_words[o_start:o_end]),
                    "object_ids":   o_chunk_ids,
                    "object_pos":   o_pos,
                    "negated":      negated
                })

    return structured_triples


# ================== MAIN ==================
def main(out: Path = TRIPLE_OUT, verbose: bool = False) -> None:
    print_headers("SUBJECT-VERB-OBJECT TRIPLE EXTRACTION", "=", prefix="\n")

    # Register custom attribute for global token IDs
    from spacy.tokens import Token
    Token.set_extension("global_id", default=None, force=True)
    
    # --------- Load tokens ---------
    print_information(f"Loading tokens from {TOKENS}...", 1, '\n')
    df = load_booknlp_file(TOKENS)
    print_information(f"Loaded {df.shape[0]} tokens", prefix="    ")

    # --------- Convert sentences to spaCy.Doc objects ---------
    print_information('Converting sentences to spaCy.Doc objects...', 2, '\n')
    docs = []
    for _, sent_df in df.groupby("sentence_ID"):
        doc = make_doc_from_sentence(sent_df.reset_index(drop=True))
        docs.append(doc)
    print_information(f"Converted {len(docs)} sentences", prefix="    ")

    # --------- Extract SVO triples ---------
    print_information('Extracting SVO triples...', 3, '\n')
    all_triples = []
    for doc in docs:
        all_triples.extend(extract_svo(doc))
    print_information(f"Extracted {len(all_triples)} triples", prefix="    ")

    # --------- Save raw SVO triples ---------
    print_information('Saving raw SVO triples...', 4, '\n')
    misc_out = out / 'misc'
    if not os.path.isdir(misc_out):
        os.mkdir(misc_out)

    # Write CSV - need to flatten tuples for CSV format
    with open(f"{misc_out}/raw_svo_triples.csv", "w", newline="", encoding="utf8") as f:
        writer = csv.DictWriter(
            f,
            delimiter='\t',
            fieldnames=["subject_text", "subject_ids", "subject_pos", "verb_text", "verb_lemma", "verb_id", 
                       "object_text", "object_ids", "object_pos", "negated"]
        )
        writer.writeheader()
        
        for triple in all_triples:
            writer.writerow(triple)

    print_information(f"Raw results saved to → {misc_out}", symb="✓")

    # --------- Match Names ---------
    print_information('Matching token IDs to character COREF IDs...', 5, '\n')
    svo = pd.read_csv(f'{misc_out}/raw_svo_triples.csv', sep='\t')
    entities = load_booknlp_file(ENTITY)
    token_to_char_id = build_token_to_character_map(entities)

    # Apply mapping to subject_ids and object_ids columns
    svo['subject_ids'] = svo['subject_ids'].apply(lambda x: map_token_ids_to_coref(x, token_to_char_id))
    svo['object_ids'] = svo['object_ids'].apply(lambda x: map_token_ids_to_coref(x, token_to_char_id))

    # Collect COREF matching statistics
    coref_stats = {
        'total_triples': len(svo),
        'subj_with_coref': svo['subject_ids'].apply(lambda x: len(x) > 0).sum(),
        'obj_with_coref': svo['object_ids'].apply(lambda x: len(x) > 0).sum(),
        'both_with_coref': ((svo['subject_ids'].apply(lambda x: len(x) > 0)) & 
                           (svo['object_ids'].apply(lambda x: len(x) > 0))).sum(),
        'unique_coref_ids': len(set(
            [id for ids in svo['subject_ids'] for id in ids] + 
            [id for ids in svo['object_ids'] for id in ids]
        ))
    }

    # Save the COREF-matched SVO triples (intermediate result)
    svo.to_csv(f'{misc_out}/svo_triples_coref.csv', sep='\t', index=False)
    print_information(f"COREF-matched SVO triples saved to → {misc_out}/svo_triples_coref.csv", symb="✓")

    # --------- Match to Canonical Names ---------
    svo, name_stats = match_to_canonical_names(svo, verbose)
    
    # Save the final matched SVO triples
    svo.to_csv(f'{out}/svo_triples.csv', sep='\t', index=False)
    print_information(f"Final matched SVO triples saved to → {out}/svo_triples.csv", symb="✓", prefix="\n", col="GREEN")

    if verbose:
        # COREF matching statistics
        print_headers("COREF MATCHING", "-", prefix="\n")
        print(f"    Triples with subject COREF: {coref_stats['subj_with_coref']} ({coref_stats['subj_with_coref']/coref_stats['total_triples']*100:.1f}%)")
        print(f"    Triples with object COREF:  {coref_stats['obj_with_coref']} ({coref_stats['obj_with_coref']/coref_stats['total_triples']*100:.1f}%)")
        print(f"    Triples with both COREF:    {coref_stats['both_with_coref']} ({coref_stats['both_with_coref']/coref_stats['total_triples']*100:.1f}%)")
        print(f"    Unique COREF IDs found:     {coref_stats['unique_coref_ids']}")
        
        # Name matching statistics
        print_headers("NAME MATCHING", "-", prefix="\n")
        print(f"    Triples before matching:    {name_stats['triples_before_matching']}")
        print(f"    Triples after matching:     {name_stats['triples_after_matching']} ({name_stats['triples_after_matching']/name_stats['triples_before_matching']*100:.1f}%)")
        print(f"    Unmatched (removed):        {name_stats['unmatched_count']}")
        print(f"    Canonical mappings used:    {name_stats['canonical_mappings_used']}")
        print(f"    Unique subjects:            {name_stats['unique_subjects']}")
        print(f"    Unique objects:             {name_stats['unique_objects']}")
        print(f"    Unique characters total:    {name_stats['unique_characters']}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Extract SVO triples from tokenized text and match to canonical names.")
    parser.add_argument(
        "-o",
        "--out",
        default=BASE_OUT_DIR,
        help="The output directory to which the processing results are written",
    )

    parser.add_argument(
        '-v', '--verbose', 
        action='store_true', 
        help="Show detailed statistics"
        )
    
    args = parser.parse_args()
    
    main(args.out, verbose=args.verbose)